{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-11T12:01:48.543417Z","iopub.execute_input":"2021-06-11T12:01:48.543862Z","iopub.status.idle":"2021-06-11T12:01:48.557766Z","shell.execute_reply.started":"2021-06-11T12:01:48.543819Z","shell.execute_reply":"2021-06-11T12:01:48.556526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Steps\n1. data preprocessing \n* importing the files(Dataset)\n* handeling missing data\n* splitting the dataset into training set and test set\n* encoding categorical data\n* Feature scaling\n2. model training\n3. model evaluation","metadata":{}},{"cell_type":"markdown","source":"# Importing dataset","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\", index_col='PassengerId')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:21.924542Z","iopub.execute_input":"2021-06-11T12:03:21.925165Z","iopub.status.idle":"2021-06-11T12:03:21.954279Z","shell.execute_reply.started":"2021-06-11T12:03:21.925123Z","shell.execute_reply":"2021-06-11T12:03:21.953298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:24.61816Z","iopub.execute_input":"2021-06-11T12:03:24.618576Z","iopub.status.idle":"2021-06-11T12:03:24.64442Z","shell.execute_reply.started":"2021-06-11T12:03:24.618539Z","shell.execute_reply":"2021-06-11T12:03:24.64333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dropping some coluoms of our tables","metadata":{}},{"cell_type":"code","source":"z=train_data.drop(['Name','Ticket','Cabin'],axis=1)\nt=test_data.drop(['PassengerId','Name','Ticket','Cabin'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:26.211194Z","iopub.execute_input":"2021-06-11T12:03:26.211583Z","iopub.status.idle":"2021-06-11T12:03:26.219454Z","shell.execute_reply.started":"2021-06-11T12:03:26.211551Z","shell.execute_reply":"2021-06-11T12:03:26.218151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:28.2179Z","iopub.execute_input":"2021-06-11T12:03:28.218306Z","iopub.status.idle":"2021-06-11T12:03:28.234364Z","shell.execute_reply.started":"2021-06-11T12:03:28.218272Z","shell.execute_reply":"2021-06-11T12:03:28.23309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:30.430186Z","iopub.execute_input":"2021-06-11T12:03:30.430738Z","iopub.status.idle":"2021-06-11T12:03:30.450172Z","shell.execute_reply.started":"2021-06-11T12:03:30.43069Z","shell.execute_reply":"2021-06-11T12:03:30.449004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* x is the matrix of the features\n* y is the independent variable\n* iloc is used to locate index (Rows and Columns)","metadata":{}},{"cell_type":"code","source":"x=z.iloc[:,1:]   \ny=train_data.iloc[:,0].values","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:38.550395Z","iopub.execute_input":"2021-06-11T12:03:38.550814Z","iopub.status.idle":"2021-06-11T12:03:38.557416Z","shell.execute_reply.started":"2021-06-11T12:03:38.550766Z","shell.execute_reply":"2021-06-11T12:03:38.556118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check if there is a missing data\nthere is 2 ways how to handle our missing data which is either by:\n* removing it\n* replacing it by -Average or - median or - most used,the most classic way to handle this is to get average","metadata":{}},{"cell_type":"code","source":"x.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:41.256494Z","iopub.execute_input":"2021-06-11T12:03:41.257068Z","iopub.status.idle":"2021-06-11T12:03:41.267371Z","shell.execute_reply.started":"2021-06-11T12:03:41.257027Z","shell.execute_reply":"2021-06-11T12:03:41.266232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking \nt.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:43.168056Z","iopub.execute_input":"2021-06-11T12:03:43.168443Z","iopub.status.idle":"2021-06-11T12:03:43.178054Z","shell.execute_reply.started":"2021-06-11T12:03:43.168411Z","shell.execute_reply":"2021-06-11T12:03:43.177045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handeling the missing data found","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\ntheimputer = SimpleImputer(missing_values=np.nan,strategy='median')\ntheimputer.fit(x.iloc[:,2:3])\nx.iloc[:,2:3]=theimputer.transform(x.iloc[:,2:3])\n#filling the missing data of Embarked coloum\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimp.fit(x.iloc[:,[6]])\nx.iloc[:,[6]]=imp.transform(x.iloc[:,[6]])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:45.181986Z","iopub.execute_input":"2021-06-11T12:03:45.182405Z","iopub.status.idle":"2021-06-11T12:03:45.205235Z","shell.execute_reply.started":"2021-06-11T12:03:45.182366Z","shell.execute_reply":"2021-06-11T12:03:45.204193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking \nx.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:47.501675Z","iopub.execute_input":"2021-06-11T12:03:47.502109Z","iopub.status.idle":"2021-06-11T12:03:47.511388Z","shell.execute_reply.started":"2021-06-11T12:03:47.502071Z","shell.execute_reply":"2021-06-11T12:03:47.510289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"filling the missing data of Age and fare coloum","metadata":{}},{"cell_type":"code","source":"#filling the missing data of Age coloum\ntheimputer.fit(t.iloc[:,[2]])\nt.iloc[:,[2]]=theimputer.transform(t.iloc[:,[2]])\n#filling the missing data of Fare coloum using mean strategy\ntheimputern2 = SimpleImputer(missing_values=np.nan,strategy='mean')\ntheimputern2.fit(t.iloc[:,[5]])\nt.iloc[:,[5]]=theimputern2.transform(t.iloc[:,[5]])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:49.146772Z","iopub.execute_input":"2021-06-11T12:03:49.147182Z","iopub.status.idle":"2021-06-11T12:03:49.169113Z","shell.execute_reply.started":"2021-06-11T12:03:49.147149Z","shell.execute_reply":"2021-06-11T12:03:49.168196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:51.145474Z","iopub.execute_input":"2021-06-11T12:03:51.146223Z","iopub.status.idle":"2021-06-11T12:03:51.154594Z","shell.execute_reply.started":"2021-06-11T12:03:51.146181Z","shell.execute_reply":"2021-06-11T12:03:51.153898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:52.755353Z","iopub.execute_input":"2021-06-11T12:03:52.755733Z","iopub.status.idle":"2021-06-11T12:03:52.769244Z","shell.execute_reply.started":"2021-06-11T12:03:52.755701Z","shell.execute_reply":"2021-06-11T12:03:52.768308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:54.354598Z","iopub.execute_input":"2021-06-11T12:03:54.356305Z","iopub.status.idle":"2021-06-11T12:03:54.372822Z","shell.execute_reply.started":"2021-06-11T12:03:54.356259Z","shell.execute_reply":"2021-06-11T12:03:54.371771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data into train and test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:56.140073Z","iopub.execute_input":"2021-06-11T12:03:56.140756Z","iopub.status.idle":"2021-06-11T12:03:56.147513Z","shell.execute_reply.started":"2021-06-11T12:03:56.140715Z","shell.execute_reply":"2021-06-11T12:03:56.146774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding Categorical data\nIn Machine learning we are require that input and output variables are numbers\n* Here we have the sex Column containg letters soit must be encoded to numbers before we can use it to fit and evaluate our mode","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\ncolumn=ColumnTransformer(transformers=[('encoder', OneHotEncoder(), ['Sex','Embarked'])],remainder='passthrough')\nxtrain= np.array(column.fit_transform(xtrain))\nxtest=np.array(column.fit_transform(xtest))\n#Encoding Categorical data\nt= np.array(column.fit_transform(t))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:57.754819Z","iopub.execute_input":"2021-06-11T12:03:57.755461Z","iopub.status.idle":"2021-06-11T12:03:57.778173Z","shell.execute_reply.started":"2021-06-11T12:03:57.755423Z","shell.execute_reply":"2021-06-11T12:03:57.777024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(xtrain)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:03:59.137126Z","iopub.execute_input":"2021-06-11T12:03:59.137863Z","iopub.status.idle":"2021-06-11T12:03:59.145061Z","shell.execute_reply.started":"2021-06-11T12:03:59.137785Z","shell.execute_reply":"2021-06-11T12:03:59.143768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(xtest)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:01:49.050088Z","iopub.execute_input":"2021-06-11T12:01:49.050498Z","iopub.status.idle":"2021-06-11T12:01:49.060438Z","shell.execute_reply.started":"2021-06-11T12:01:49.050458Z","shell.execute_reply":"2021-06-11T12:01:49.059129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(t)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:01:49.063492Z","iopub.execute_input":"2021-06-11T12:01:49.064078Z","iopub.status.idle":"2021-06-11T12:01:49.075025Z","shell.execute_reply.started":"2021-06-11T12:01:49.064014Z","shell.execute_reply":"2021-06-11T12:01:49.073668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature scaling ","metadata":{}},{"cell_type":"code","source":"#Feature scaling is essential as the range of all features should be in fixed range\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n#features to be scaled : age and fare\nxtrain= sc.fit_transform(xtrain)\nxtest = sc.transform(xtest)\n#Scaling the data of test\nt = sc.transform(t)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:04:01.172404Z","iopub.execute_input":"2021-06-11T12:04:01.173206Z","iopub.status.idle":"2021-06-11T12:04:01.179762Z","shell.execute_reply.started":"2021-06-11T12:04:01.173161Z","shell.execute_reply":"2021-06-11T12:04:01.178899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"#For my model Random Forest Classifier\n#To produce a more accurate result, it builds numerous decision trees and then merges them together.\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 1)\nclassifier.fit(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:04:04.458514Z","iopub.execute_input":"2021-06-11T12:04:04.45926Z","iopub.status.idle":"2021-06-11T12:04:04.519326Z","shell.execute_reply.started":"2021-06-11T12:04:04.45922Z","shell.execute_reply":"2021-06-11T12:04:04.518563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ypredict = classifier.predict(xtest)\nypredict\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:04:07.030198Z","iopub.execute_input":"2021-06-11T12:04:07.030918Z","iopub.status.idle":"2021-06-11T12:04:07.04204Z","shell.execute_reply.started":"2021-06-11T12:04:07.030879Z","shell.execute_reply":"2021-06-11T12:04:07.041188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"#Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nconfussionMatrix = confusion_matrix(ytest, ypredict)\nprint(confussionMatrix)\naccuracy_score(ytest, ypredict)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:04:08.695305Z","iopub.execute_input":"2021-06-11T12:04:08.695991Z","iopub.status.idle":"2021-06-11T12:04:08.705817Z","shell.execute_reply.started":"2021-06-11T12:04:08.695946Z","shell.execute_reply":"2021-06-11T12:04:08.704968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = classifier.predict(t)\npredict","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:04:10.198385Z","iopub.execute_input":"2021-06-11T12:04:10.198999Z","iopub.status.idle":"2021-06-11T12:04:10.215401Z","shell.execute_reply.started":"2021-06-11T12:04:10.198947Z","shell.execute_reply":"2021-06-11T12:04:10.214244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# submission","metadata":{}},{"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predict})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T12:04:12.392978Z","iopub.execute_input":"2021-06-11T12:04:12.39366Z","iopub.status.idle":"2021-06-11T12:04:12.403745Z","shell.execute_reply.started":"2021-06-11T12:04:12.393621Z","shell.execute_reply":"2021-06-11T12:04:12.402402Z"},"trusted":true},"execution_count":null,"outputs":[]}]}